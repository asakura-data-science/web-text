{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5SWfJD-yBhFI"
      },
      "source": [
        "# 3章. テキスト分析1：テキストのベクトル化\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yOd4q99kBhFN"
      },
      "source": [
        "## 3.1 テキスト分析の目的\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt -y install fonts-ipafont-gothic\n",
        "!pip install spacy[ja]\n"
      ],
      "metadata": {
        "id": "lU3sTtAkGZYl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NetqrD9lBhFP"
      },
      "outputs": [],
      "source": [
        "# 本章で用いる各種モジュールのインポート。\n",
        "import os\n",
        "import urllib.request\n",
        "import tarfile\n",
        "import math\n",
        "import re\n",
        "import unicodedata\n",
        "import platform\n",
        "import random\n",
        "import glob\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from spacy.lang.ja import Japanese\n",
        "from wordcloud import WordCloud\n",
        "from IPython.display import Image\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.font_manager import FontProperties\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RasaIzgoBhFV"
      },
      "outputs": [],
      "source": [
        "# 本章で用いる各種設定。\n",
        "\n",
        "# データを保持しておくディレクトリ。\n",
        "DATA_DIR = \"data\"\n",
        "# livedoorニュースコーパスのURL。\n",
        "LDCC_URL = \"https://www.rondhuit.com/download/ldcc-20140209.tar.gz\"\n",
        "# livedoorニュースコーパスを保存するパス。\n",
        "LDCC_FILE = os.path.join(DATA_DIR, \"ldcc-20140209.tar.gz\")\n",
        "# livedoorニュースコーパスを解凍するディレクトリ。\n",
        "NEWS_DIR = os.path.join(DATA_DIR, \"text\")\n",
        "# データフレームを保存するCSVファイル。\n",
        "CSV_PATH = os.path.join(DATA_DIR, \"newsdf.csv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# livedoorニュースコーパスの入手と解凍。\n",
        "# データディレクトリの作成を行う。\n",
        "os.makedirs(DATA_DIR, exist_ok=True)\n",
        "# ファイルのダウンロードを行う。\n",
        "req = urllib.request.Request(LDCC_URL)\n",
        "with urllib.request.urlopen(req) as res:\n",
        "    data = res.read()\n",
        "    with open(LDCC_FILE, mode=\"wb\") as f:\n",
        "        f.write(data)\n",
        "# ファイルの解凍を行う。\n",
        "with tarfile.open(LDCC_FILE) as tar:\n",
        "    tar.extractall(DATA_DIR)\n"
      ],
      "metadata": {
        "id": "1CoRlHVuYGo9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lTsLI_WWBhFb"
      },
      "outputs": [],
      "source": [
        "# livedoorニュースコーパスのトピック情報。\n",
        "category_names = [\"ITライフハック\", \"MOVIE ENTER\",\n",
        "    \"Peachy\", \"Sports Watch\", \"livedoor HOMME\",\n",
        "    \"エスマックス\", \"トピックニュース\",\n",
        "    \"家電チャンネル\", \"独女通信\"]\n",
        "\n",
        "# livedoorニュースコーパスのトピックに対応するディレクトリ名。\n",
        "category_dirs = [\"it-life-hack\", \"movie-enter\",\n",
        "    \"peachy\", \"sports-watch\", \"livedoor-homme\",\n",
        "    \"smax\", \"topic-news\",\n",
        "    \"kaden-channel\", \"dokujo-tsushin\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2e0icCsqBhFe"
      },
      "outputs": [],
      "source": [
        "# 全ての記事のテキストファイルを読み込む。\n",
        "docs = []\n",
        "for cat in category_dirs:\n",
        "    pattern = os.path.join(NEWS_DIR, cat, f\"{cat}*.txt\")\n",
        "    for src_file in sorted(glob.glob(pattern)):\n",
        "        with open(src_file, \"r\", encoding=\"utf8\") as f:\n",
        "            url = f.readline().strip()\n",
        "            date = f.readline().strip()\n",
        "            title = f.readline().strip()\n",
        "            body = f.read().strip()\n",
        "        docs.append((cat, url, date, title, body))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_O-qBpobBhFf"
      },
      "outputs": [],
      "source": [
        "# 総記事数を表示する。\n",
        "num_docs = len(docs)\n",
        "print(f\"総記事数：{num_docs}件\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "S7Reya2cBhFi"
      },
      "outputs": [],
      "source": [
        "# DataFrameに格納して最初の5件の文書を表示する。\n",
        "df = pd.DataFrame(docs, columns=[\"category\", \"url\", \"date\",\n",
        "                                 \"title\", \"body\"])\n",
        "display(df.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "SBFNlzJ9BhFk"
      },
      "outputs": [],
      "source": [
        "# インデックス0の文書のタイトルを表示。\n",
        "print(df.iloc[0][\"title\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "yyhBl3b_BhFl"
      },
      "outputs": [],
      "source": [
        "# インデックス0の文書の本文を表示。\n",
        "print(df.iloc[0][\"body\"])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "naperrqhBhFm"
      },
      "source": [
        "## 3.2 文書の前処理\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "CKiRrn5SBhFn"
      },
      "outputs": [],
      "source": [
        "# 形態素解析の準備とその利用。\n",
        "\n",
        "# 形態素解析器の準備。\n",
        "nlp = Japanese()\n",
        "# インデックス0の文書のタイトルを形態素解析する。\n",
        "tokens = nlp(df.iloc[0][\"title\"])\n",
        "print(\"トークン番号, 表層形, 品詞, 品詞細分類, 原型\")\n",
        "for token in tokens:\n",
        "    print(f\"{token.i:>2}, {token.orth_}, {token.pos_}, {token.tag_}, {token.lemma_}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aPoQLwXFBhFo"
      },
      "outputs": [],
      "source": [
        "# 関数tokenizeの定義。\n",
        "def tokenize(text):\n",
        "    tokens = nlp(text)\n",
        "    result = []\n",
        "    for token in tokens:\n",
        "        result.append(token.orth_)\n",
        "    return \" \".join(result)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Js6uv3EZBhFp"
      },
      "outputs": [],
      "source": [
        "# 関数tokenizeの利用例。\n",
        "print(tokenize(\"すもももももももものうち。\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NSJFyJAlBhF4"
      },
      "outputs": [],
      "source": [
        "# 関数get_font_pathの定義の例。インストールされているフォントファイルを指定する必要がある。\n",
        "def get_font_path():\n",
        "    # OSによってフォントを使い分ける。\n",
        "    pf = platform.system()\n",
        "    if pf == \"Windows\":\n",
        "        return r\"C:\\Windows\\Fonts\\meiryo.ttc\"\n",
        "    elif pf == \"Darwin\":\n",
        "        return \"/System/Library/Fonts/ヒラギノ角ゴシック W3.ttc\"\n",
        "    elif pf == \"Linux\":\n",
        "        return \"/usr/share/fonts/opentype/ipafont-gothic/ipagp.ttf\"\n",
        "    else:\n",
        "        raise RuntimeError(\"対応していません。\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "gjdjZimPBhF4"
      },
      "outputs": [],
      "source": [
        "# 関数show_wordcloudの定義。\n",
        "def show_wordcloud(words):\n",
        "    # 日本語フォントを取得する。\n",
        "    font_path = get_font_path()\n",
        "    # 分割テキストからwordcloudを生成する。\n",
        "    wordcloud = WordCloud(font_path=font_path, background_color=\"white\")\n",
        "    wordcloud.generate(words)\n",
        "    # 表示する。\n",
        "    wordcloud.to_file(f\"./wordcloud.png\")\n",
        "    display(Image(f\"./wordcloud.png\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iu3rmcfOBhF6"
      },
      "outputs": [],
      "source": [
        "# インデックス0の文書を分かち書きして最初の100語を表示する。\n",
        "words = tokenize(df.iloc[0][\"body\"])\n",
        "print(words[0:100])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RwJp40V3BhF6"
      },
      "outputs": [],
      "source": [
        "# インデックス0の文書の全ての語によるワードクラウドを表示する。\n",
        "show_wordcloud(words)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-cM5C1YnBhF8"
      },
      "outputs": [],
      "source": [
        "# テキストのクリーニングを行う関数text_cleaningの定義。\n",
        "\n",
        "re_url = re.compile(r\"https?://[;/\\?:@&=\\+\\$,0-9A-Za-z\\-_\\.!~\\*\\'\\(\\)%#]+\")\n",
        "re_num = re.compile(r\"\\d([\\d.,]?\\d)*\")\n",
        "\n",
        "def text_cleaning(text):\n",
        "    # 互換等価性変換を実行する\n",
        "    text = unicodedata.normalize(\"NFKC\", text)\n",
        "    # URLを削除する。\n",
        "    text = re_url.sub(\"\", text)\n",
        "    # 全ての数値を0にする。\n",
        "    text = re_num.sub(\"0\", text)\n",
        "    # 英語を全て小文字にする場合は、次行のコメントを外す。\n",
        "    #text = text.lower()\n",
        "    return text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "zZJdFEqYBhF9"
      },
      "outputs": [],
      "source": [
        "# テキストのクリーニングの実行。\n",
        "text = text_cleaning(df.iloc[0][\"body\"])\n",
        "print(text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WX_p7F6NBhF_"
      },
      "outputs": [],
      "source": [
        "# 関数extract_wordsの定義。\n",
        "def extract_words(text):\n",
        "    # テキストクリーニングを行った結果を形態素解析する。\n",
        "    tokens = nlp(text_cleaning(text))\n",
        "    result = []\n",
        "    target_pos_set = set((\"NOUN\", \"PROPN\", \"VERB\", \"ADJ\", \"ADV\"))\n",
        "    for token in tokens:\n",
        "        # 対象となる品詞かどうかをチェックする。\n",
        "        if token.pos_ in target_pos_set:\n",
        "            # 原型を取得する。\n",
        "            result.append(token.lemma_)\n",
        "    return \" \".join(result)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tfLOKlfVBhGA"
      },
      "outputs": [],
      "source": [
        "# インデックス0の文書の名詞や動詞などのみを対象としてワードクラウドを表示する。\n",
        "words = extract_words(df.iloc[0][\"body\"])\n",
        "show_wordcloud(words)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# df[\"words\"]に分かち書きを追加する。\n",
        "df = df.assign(words=(df[\"title\"] + df[\"body\"]).apply(text_cleaning).apply(extract_words))\n",
        "# CSVファイルに保存する。\n",
        "df.to_csv(CSV_PATH)\n"
      ],
      "metadata": {
        "id": "5oYrFLYQxTq6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "d5sa0WruBhGD"
      },
      "outputs": [],
      "source": [
        "# 更新されたデータフレームの表示。\n",
        "display(df.head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V8fgNt0XBhGF"
      },
      "source": [
        "## 3.3 語の出現頻度を基にしたテキストのベクトル化と類似度計算\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oaDmh3BdBhGG"
      },
      "outputs": [],
      "source": [
        "# 語の出現頻度による特徴ベクトルの作成。\n",
        "\n",
        "# 語の出現頻度による特徴ベクトルを作成するクラスを用意する。\n",
        "vectorizer_count = CountVectorizer(token_pattern=r\"(?u)\\b\\w+\\b\")\n",
        "# 全文書を特徴ベクトル化する。\n",
        "vectors_count = vectorizer_count.fit_transform(df[\"words\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "R5_6WSudBhGH"
      },
      "outputs": [],
      "source": [
        "# CountVectorizerの設定の表示。\n",
        "display(vectorizer_count.get_params())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "u5r7aWMSBhGI"
      },
      "outputs": [],
      "source": [
        "# 文書の特徴ベクトルの変数の概要を確認する。\n",
        "display(vectors_count)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pICH-byVBhGJ"
      },
      "outputs": [],
      "source": [
        "# どのような語が次元になったのかを取得する。\n",
        "vocabulary = vectorizer_count.get_feature_names_out().tolist()\n",
        "# 語の種類数を表示する。\n",
        "print(f\"語の種類数：{len(vocabulary)}\")\n",
        "# ランダムに6語表示する。\n",
        "print(random.sample(vocabulary, 6))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "EFa76RaiBhGK"
      },
      "outputs": [],
      "source": [
        "# インデックス0の文書ではどのような語の出現回数が多いかを表示する。\n",
        "doc_id = 0\n",
        "elements = zip(vectors_count[doc_id].data, vectors_count[doc_id].indices)\n",
        "elements = sorted(elements, reverse=True)\n",
        "\n",
        "# 出現回数が多い語と出現回数を表示\n",
        "for i in range(min(len(elements), 5)):\n",
        "    print(f\"文書（{doc_id}）での語「{vocabulary[elements[i][1]]}」の出現回数は{elements[i][0]}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LKQWscA0BhGL"
      },
      "outputs": [],
      "source": [
        "# 語「神戸」がどの文書で出現頻度が多いかを表示する。\n",
        "target_word = \"神戸\"\n",
        "target_word_id = vocabulary.index(target_word)\n",
        "\n",
        "# wordが1回以上出現している文書のインデックスを全て取得する。\n",
        "doc_ids = [i for i, v in enumerate(vectors_count) if target_word_id in v.indices]\n",
        "\n",
        "elements = []\n",
        "for doc_id in doc_ids:\n",
        "    # その文書の特徴ベクトルを取得する。ただし、粗行列表現である。\n",
        "    vector = vectors_count[doc_id]\n",
        "    # 粗行列表現の何番目が与えられたwordに対応しているかを求める。\n",
        "    word_id = vector.indices.tolist().index(target_word_id)\n",
        "    # 与えられたwordの出現頻度を取得する。\n",
        "    count = vector.data[word_id]\n",
        "    # 並べ替える要素に追加する。\n",
        "    elements.append((count, doc_id))\n",
        "\n",
        "# 出現回数が多い順に並べ替える。\n",
        "elements = sorted(elements, reverse=True)\n",
        "\n",
        "# wordの出現回数が多い文書のインデックスと出現回数を表示\n",
        "for i in range(min(len(elements), 5)):\n",
        "    print(f\"文書（{elements[i][1]}）での語「{target_word}」の出現回数は{elements[i][0]}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "AdSGRa9xBhGM"
      },
      "outputs": [],
      "source": [
        "# インデックス3114の文書のタイトルと本文を表示する。\n",
        "print(df.iloc[3114][\"title\"])\n",
        "print(df.iloc[3114][\"body\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FMoKjwY8BhGO"
      },
      "outputs": [],
      "source": [
        "# 関数show_wordcloud_by_vector\n",
        "def show_wordcloud_by_vector(vector, vocabulary):\n",
        "    # 日本語フォントを取得する。\n",
        "    font_path = get_font_path()\n",
        "    # 語とその重みを表す辞書を作成する。\n",
        "    vector_dict = {}\n",
        "    # ベクトルは粗行列で与えられる場合とndarrayで与えられる場合を想定する。\n",
        "    if not isinstance(vector, np.ndarray):\n",
        "        vector = vector.toarray()[0]\n",
        "    for word_id in vector.nonzero()[0]:\n",
        "        vector_dict[vocabulary[word_id]] = vector[word_id]\n",
        "    # 語とその重みを表す辞書からwordcloudを生成する。\n",
        "    wordcloud = WordCloud(font_path=font_path, background_color=\"white\")\n",
        "    wordcloud.generate_from_frequencies(vector_dict)\n",
        "    # 表示する。\n",
        "    wordcloud.to_file(\"./wordcloud.png\")\n",
        "    display(Image(\"./wordcloud.png\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "10fV5He7BhGQ"
      },
      "outputs": [],
      "source": [
        "# インデックス3114の文書のワードクラウドを表示する。\n",
        "show_wordcloud_by_vector(vectors_count[3114], vocabulary)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8X-bUYJDBhGR"
      },
      "outputs": [],
      "source": [
        "# インデックス0の文書とインデックス3114の文書のコサイン類似度を計算する。\n",
        "cos = cosine_similarity(vectors_count[0], vectors_count[3114])\n",
        "print(cos)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lfjbrSUTBhGS"
      },
      "outputs": [],
      "source": [
        "# インデックス3114の文書の特徴ベクトルの概要を表示する。\n",
        "display(vectors_count[3114])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "unNiMCk7BhGS"
      },
      "outputs": [],
      "source": [
        "# インデックス3114の文書の特徴ベクトルをndarrayで取得する。\n",
        "v3114 = vectors_count[3114].toarray()[0]\n",
        "print(v3114)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QCWizfoSBhGU"
      },
      "outputs": [],
      "source": [
        "v0 = vectors_count[0].toarray()[0]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QR_ejfeMBhGW"
      },
      "outputs": [],
      "source": [
        "# インデックス0の文書とインデックス3114の文書の特徴ベクトルからコサイン類似度を計算する。\n",
        "print(cosine_similarity([v0], [v3114]))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vnVq5yZeBhGX"
      },
      "outputs": [],
      "source": [
        "# 5つの文書の全ての組み合わせについてコサイン類似度を計算する。\n",
        "doc_ids = [3114, 1942, 1920, 2067, 3128]\n",
        "sim_matrix = cosine_similarity(vectors_count[doc_ids],\n",
        "                               vectors_count[doc_ids])\n",
        "display(pd.DataFrame(sim_matrix, index=doc_ids, columns=doc_ids))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "otz-hzoFBhGY"
      },
      "outputs": [],
      "source": [
        "# 5つの記事のタイトル（最初の25文字）を表示する。\n",
        "for doc_id in doc_ids:\n",
        "    print(f\"{doc_id}: {df.iloc[doc_id]['title'][:25]}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nV48Lgu6BhGZ"
      },
      "outputs": [],
      "source": [
        "# 2つの新しい文書\n",
        "newdocs = [\n",
        "    \"今日は朝から晴れています。\",\n",
        "    \"明日は霧雨が降るそうです。\"\n",
        "]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eUurX28GBhGZ"
      },
      "outputs": [],
      "source": [
        "# 2つの新しい文書から語の抽出を行う。\n",
        "newdocs_words = list(map(extract_words, newdocs))\n",
        "print(newdocs_words)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "hQWqQMcxBhGa"
      },
      "outputs": [],
      "source": [
        "# 2つの新しい文書の特徴ベクトルを取得する。\n",
        "newdocs_vectors = vectorizer_count.transform(newdocs_words)\n",
        "print(newdocs_vectors)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JOkuJI3eBhGa"
      },
      "outputs": [],
      "source": [
        "# 新しい文書における語と出現頻度を表示する。\n",
        "for i in range(len(newdocs)):\n",
        "    vector = newdocs_vectors[i]\n",
        "    for count, word_id in zip(vector.data, vector.indices):\n",
        "        print(f\"新しい文書（{i}）での語「{vocabulary[word_id]}」の出現頻度は{count}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yskrTpFBBhGb"
      },
      "outputs": [],
      "source": [
        "# 簡易検索システムを実現する関数search_by_queryを定義する。\n",
        "def search_by_query(query, vectorizer, vectors):\n",
        "    print(\"■■■■■■■■■■■検索■■■■■■■■■■■■\")\n",
        "    # クエリ文字列の前処理を行い、ベクトル化する。\n",
        "    query_words = extract_words(query)\n",
        "    query_vector = vectorizer.transform([query_words])\n",
        "    print(\"検索クエリ: {}\".format(query_words))\n",
        "\n",
        "    # クエリのベクトルととすべて文書のコサイン類似度を求める。\n",
        "    sims = cosine_similarity(query_vector, vectors)\n",
        "    # コサイン類似度と、文書インデックスのペアを作成する。\n",
        "    sim_idx_pairs = zip(sims[0], range(vectors.shape[0]))\n",
        "    # コサイン類似度が高い順にソートする。\n",
        "    ranking_result = sorted(sim_idx_pairs, reverse=True)\n",
        "\n",
        "    # 検索結果のトップ3件を表示する。\n",
        "    print(\"■■■■■■■■■■検索結果■■■■■■■■■■■\")\n",
        "    for i in range(3):\n",
        "        cos = ranking_result[i][0]\n",
        "        doc_id = ranking_result[i][1]\n",
        "        print(f\"＝＝＝＝＝＝＝＝＝＝＝第{i+1}位＝＝＝＝＝＝＝＝＝＝＝\")\n",
        "        print(f\"★記事ID：{doc_id}／コサイン類似度：{cos:.3f}\")\n",
        "        print(df.iloc[doc_id][\"url\"])\n",
        "        print(df.iloc[doc_id][\"title\"][:25])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "3ck9gc7WBhGd"
      },
      "outputs": [],
      "source": [
        "# 「グルメ レストラン」で検索を行う。\n",
        "search_by_query(\"グルメ レストラン\", vectorizer_count, vectors_count)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "50xvpejuBhGe"
      },
      "outputs": [],
      "source": [
        "# TF-IDF重み付けによる特徴ベクトルの作成。\n",
        "\n",
        "# TF-IDF重み付けによる特徴ベクトルを作成するクラスを用意する。\n",
        "vectorizer_tfidf = TfidfVectorizer(norm=None,\n",
        "                                   token_pattern=r\"(?u)\\b\\w+\\b\")\n",
        "# 全文書を特徴ベクトル化する。\n",
        "vectors_tfidf = vectorizer_tfidf.fit_transform(df[\"words\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "W8DSzdFhBhGf"
      },
      "outputs": [],
      "source": [
        "# TfidfVectorizerの設定の表示。\n",
        "display(vectorizer_tfidf.get_params())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "ts1JmOmnBhGh"
      },
      "outputs": [],
      "source": [
        "# IDF値による語のヒストグラムを表示する。\n",
        "plt.hist(vectorizer_tfidf.idf_, bins=11, range=(0, 11))\n",
        "# 日本語フォントを取得する。\n",
        "fp = FontProperties(fname=get_font_path())\n",
        "plt.xlabel(\"IDF値\", fontproperties=fp)\n",
        "plt.ylabel(\"語数\", fontproperties=fp)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XSDodSx_BhGi"
      },
      "outputs": [],
      "source": [
        "# 変数vectors_tfidfの概要を表示する。\n",
        "display(vectors_tfidf)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3-pcJ2XoBhGi"
      },
      "outputs": [],
      "source": [
        "# どのような語が次元になったのかを取得する。\n",
        "vocabulary = vectorizer_tfidf.get_feature_names_out().tolist()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "xHaOkrLEBhGj"
      },
      "outputs": [],
      "source": [
        "# インデックス0の文書ではどのような語のTF-IDF値が大きいかを表示する。\n",
        "doc_id = 0\n",
        "elements = zip(vectors_tfidf[doc_id].data, vectors_tfidf[doc_id].indices)\n",
        "elements = sorted(elements, reverse=True)\n",
        "\n",
        "# TF-IDF値が大きい語を表示\n",
        "for i in range(min(len(elements), 5)):\n",
        "    print(f\"文書（{doc_id}）での\"\n",
        "          f\"語「{vocabulary[elements[i][1]]}」\"\n",
        "          f\"のTF-IDF重みは{elements[i][0]:.2f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j5y3VQEjBhGn"
      },
      "outputs": [],
      "source": [
        "# TF-IDF重み付けによるワードクラウドの表示。\n",
        "print(\"TF-IDF重み付けによるワードクラウド\")\n",
        "show_wordcloud_by_vector(vectors_tfidf[0], vocabulary)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "y9RO1BiMBhGp"
      },
      "outputs": [],
      "source": [
        "# 「グルメ レストラン」でTF-IDF重み付けで検索を行う。\n",
        "search_by_query(\"グルメ レストラン\", vectorizer_tfidf, vectors_tfidf)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QePUIqA4BhGp"
      },
      "source": [
        "# 4章. テキスト分析2：ベクトルを用いた分析\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-5GGZvFYBhGq"
      },
      "source": [
        "## 4.1 特徴ベクトルの次元圧縮とトピック抽出\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nm8PUMGVBhGr"
      },
      "outputs": [],
      "source": [
        "# 前章のものに加えて、本章で用いる各種モジュールのインポート。\n",
        "import seaborn as sns\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "from sklearn.metrics import adjusted_rand_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.preprocessing import normalize\n",
        "from sklearn.cluster import KMeans\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z0WOOhmGBhGt"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "# 潜在的意味解析で200次元に次元圧縮を行う。\n",
        "lsa = TruncatedSVD(200, algorithm=\"arpack\")\n",
        "vectors_lsa = lsa.fit_transform(vectors_tfidf)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "-iyJ38vpBhGu"
      },
      "outputs": [],
      "source": [
        "# 変数vectors_lsaの行列の形を表示する。\n",
        "print(vectors_lsa.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DJVNWLTPBhGu"
      },
      "outputs": [],
      "source": [
        "# インデックス0の文書の圧縮された特徴ベクトルにおける最初の3次元と最後の3次元を表示する。\n",
        "print(vectors_lsa[0][:3])\n",
        "print(vectors_lsa[0][-3:])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "idvYKBqQBhGv"
      },
      "outputs": [],
      "source": [
        "# 5つの文書のコサイン類似度の計算。\n",
        "doc_ids = [3114, 1942, 1920, 2067, 3128]\n",
        "# TF-IDF重み付けを使った特徴ベクトルの場合。\n",
        "sim_matrix_tfidf = cosine_similarity(vectors_tfidf[doc_ids], vectors_tfidf[doc_ids])\n",
        "display(pd.DataFrame(sim_matrix_tfidf, index=doc_ids, columns=doc_ids))\n",
        "# TF-IDF重み付けを使った特徴ベクトルを潜在的意味解析により200次元に圧縮した場合。\n",
        "sim_matrix_lsa = cosine_similarity(vectors_lsa[doc_ids], vectors_lsa[doc_ids])\n",
        "display(pd.DataFrame(sim_matrix_lsa, index=doc_ids, columns=doc_ids))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "suYYhJHUBhGy"
      },
      "outputs": [],
      "source": [
        "# TF-IDF重み付けと次元圧縮をこの順序で適用するためのパイプラインを作成する。\n",
        "vectorizer_lsa = make_pipeline(vectorizer_tfidf, lsa)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "pYWKvyuhBhG1"
      },
      "outputs": [],
      "source": [
        "# 「グルメ レストラン」で次元圧縮された特徴ベクトルで検索を行う。\n",
        "search_by_query(\"グルメ レストラン\", vectorizer_lsa, vectors_lsa)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3tgC9H_aBhG4"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "# LDAで50個のトピックを前提とした潜在的ディリクレ配分法の学習を行う。\n",
        "lda = LatentDirichletAllocation(n_components=50)\n",
        "vectors_lda = lda.fit_transform(vectors_tfidf)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "4GxCDR_WBhG5"
      },
      "outputs": [],
      "source": [
        "# 変数vectors_ldaの行列の形を表示する。\n",
        "print(vectors_lda.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "AYEN9QFfBhG7"
      },
      "outputs": [],
      "source": [
        "# インデックス3114の文書のトピックの分布の表示。\n",
        "print(vectors_lda[3114])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jyy9fsF8BhG8"
      },
      "outputs": [],
      "source": [
        "# インデックス3114の文書のトピックの分布の合計値の表示。\n",
        "print(vectors_lda[3114].sum())\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# インデックス3114の文書がどのようなトピックで構成されているかを表示\n",
        "for i, v in sorted(enumerate(vectors_lda[3114]), key=lambda x:x[1]):\n",
        "    if v > 0.0001:\n",
        "        print(f\"トピック{i:2}の重み：{v:.4f}\")\n",
        "max_index = i\n"
      ],
      "metadata": {
        "id": "D6c6nAHrC2NM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tz3efFIBBhG-"
      },
      "outputs": [],
      "source": [
        "# トピックの情報が格納された行列の形を表示する。\n",
        "print(lda.components_.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CQYdW1TEBhG_"
      },
      "outputs": [],
      "source": [
        "# トピックmax_indexでどのような語の重みが大きいかを表示する。\n",
        "topic_id = max_index\n",
        "# 語のインデックスと重みを対応づける。\n",
        "elements = zip(lda.components_[topic_id], vocabulary)\n",
        "elements = sorted(elements, reverse=True)\n",
        "# 重みが大きい語と重みを表示\n",
        "for i in range(min(len(elements), 5)):\n",
        "    print(f\"トピック（{topic_id}）での語「{elements[i][1]}」の重みは{elements[i][0]:.3f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ul8XB3xDBhHA"
      },
      "outputs": [],
      "source": [
        "# インデックスmax_indexのトピックのワードクラウドの表示。\n",
        "show_wordcloud_by_vector(lda.components_[topic_id], vocabulary)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "yn_P7FBTBhHC"
      },
      "outputs": [],
      "source": [
        "# トピックmax_indexの重みが大きい文書を表示する。\n",
        "topic_id = max_index\n",
        "# 全文書におけるトピックmax_indexの重みと、文書のインデックスを対応づける。\n",
        "elements = zip(vectors_lda[:,topic_id], range(num_docs))\n",
        "elements = sorted(elements, reverse=True)\n",
        "# トピックの重みが大きい文書と重みを表示\n",
        "for i in range(min(len(elements), 35)):\n",
        "    print(f\"文書（{elements[i][1]}）でのトピック（{topic_id}）の重みは{elements[i][0]:.3f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "5ln4i5IxBhHE"
      },
      "outputs": [],
      "source": [
        "# インデックス3174の文書のTF-IDF重み付けによるワードクラウドを表示する。\n",
        "show_wordcloud_by_vector(vectors_tfidf[3174], vocabulary)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DrpqHJ3xBhHH"
      },
      "source": [
        "## 4.2 文書のクラスタリング\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "slM2aqxeBhHK"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "# 最長距離法でコサイン類似度を用いてクラスタリングを行う。\n",
        "clustering_agg = AgglomerativeClustering(linkage=\"complete\", affinity=\"cosine\", compute_full_tree=True)\n",
        "clustering_agg.fit_predict(vectors_lsa)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gOaP4oBoBhHL"
      },
      "outputs": [],
      "source": [
        "# 階層クラスタリングの結果からクラスタを構成する関数の定義。\n",
        "def get_labels(num_clusters, children):\n",
        "    num_samples = len(children) + 1\n",
        "    label_dict = {i: i for i in range(num_samples)}\n",
        "    inv_label_dict = {i: {i} for i in range(num_samples)}\n",
        "    num_marge = num_samples - num_clusters\n",
        "    for cluster, (left, right) in enumerate(children[:num_marge], num_samples):\n",
        "        members = inv_label_dict.pop(left) | inv_label_dict.pop(right)\n",
        "        inv_label_dict[cluster] = members\n",
        "        for member in members:\n",
        "            label_dict[member] = cluster\n",
        "    labels = np.zeros(num_samples)\n",
        "    for cluster, members in enumerate(inv_label_dict.values()):\n",
        "        labels[list(members)] = cluster\n",
        "    return labels\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "6y1VhBfqBhHL"
      },
      "outputs": [],
      "source": [
        "# 20個のクラスタを構成する。\n",
        "labels = get_labels(20, clustering_agg.children_)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s20PqsPsBhHL"
      },
      "outputs": [],
      "source": [
        "# 得られた20個のクラスタの情報を表示する。\n",
        "print(f\"文書総数：{len(labels)}\")\n",
        "print(labels)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "VQHnWHPPBhHM"
      },
      "outputs": [],
      "source": [
        "# クラスタ数を10から150まで変化させたときの調整ランド指数の変化をみる。\n",
        "X = range(10, 151, 5)\n",
        "Y = []\n",
        "for num_clusters in X:\n",
        "    labels = get_labels(num_clusters, clustering_agg.children_)\n",
        "    Y.append(adjusted_rand_score(df[\"category\"], labels))\n",
        "# クラスタ数と調整ランド指数の折れ線グラフを描く。\n",
        "plt.plot(X, Y)\n",
        "# 日本語フォントを取得する。\n",
        "fp = FontProperties(fname=get_font_path())\n",
        "plt.xlabel(\"クラスタ数\", fontproperties=fp)\n",
        "plt.ylabel(\"調整ランド指数\", fontproperties=fp)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0hd-5qErBhHN"
      },
      "outputs": [],
      "source": [
        "# それぞれのカテゴリーとクラスタに属する文書をまとめる。\n",
        "# 30個のクラスタにクラスタリングした結果を取得する。\n",
        "num_clusters = 30\n",
        "labels = get_labels(num_clusters, clustering_agg.children_)\n",
        "# それぞれのカテゴリーとクラスタに属する文書をまとめる。\n",
        "cmt = confusion_matrix(df[\"category\"].apply(category_dirs.index).values, labels)\n",
        "cmt = cmt[:num_clusters][:len(category_dirs)]\n",
        "cmt = cmt / num_docs * 100\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "shGQ-FYOBhHO"
      },
      "outputs": [],
      "source": [
        "# カテゴリーとクラスタの関係を可視化する。\n",
        "plt.figure(figsize=(16, 8))\n",
        "sns.heatmap(cmt, annot=True, fmt=\"1.1f\", cmap=\"Blues\")\n",
        "# 日本語フォントを取得する。\n",
        "fp = FontProperties(fname=get_font_path())\n",
        "plt.xlabel(\"クラスタ\", fontproperties=fp)\n",
        "plt.ylabel(\"カテゴリー\", fontproperties=fp)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# インデックス29のクラスタのワードクラウドを表示する。\n",
        "# 【注】ここでインデックス29を選んでいますが、クラスタリングの結果を見ながら適当なクラスタを選択して下さい。\n",
        "label = 29\n",
        "doc_ids = np.where(labels==label)[0]\n",
        "doc_vectors = vectors_tfidf[doc_ids]\n",
        "mean_vector = np.asarray(doc_vectors.mean(axis=0))[0]\n",
        "show_wordcloud_by_vector(mean_vector, vocabulary)\n"
      ],
      "metadata": {
        "id": "1jP2sgW-MbfU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E2Mnrxf6BhHT"
      },
      "outputs": [],
      "source": [
        "# L2正規化を行う。\n",
        "vectors_lsa_norm = normalize(vectors_lsa, norm=\"l2\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hCPoPi-5BhHT"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "# K-Means法でクラスタリングを行う。\n",
        "num_clusters = 9\n",
        "clustering_kmeans = KMeans(n_clusters=num_clusters)\n",
        "labels = clustering_kmeans.fit_predict(vectors_lsa_norm)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "2RJmaaKJBhHV"
      },
      "outputs": [],
      "source": [
        "# K-Means法の結果の調整ランド指数を表示する。\n",
        "print(adjusted_rand_score(df[\"category\"], labels))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pf59JCy8BhHV"
      },
      "outputs": [],
      "source": [
        "# それぞれのカテゴリーとクラスタに属する文書をまとめる。\n",
        "cmt = confusion_matrix(df[\"category\"].apply(category_dirs.index).values, labels)\n",
        "cmt = cmt[:num_clusters][:len(category_dirs)]\n",
        "cmt = cmt / num_docs * 100\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "5jB5D1T1BhHV"
      },
      "outputs": [],
      "source": [
        "# カテゴリーとクラスタの関係を可視化する。\n",
        "plt.figure(figsize=(16,8))\n",
        "sns.heatmap(cmt, annot=True, fmt=\"1.1f\", cmap=\"Blues\")\n",
        "# 日本語フォントを取得する。\n",
        "fp = FontProperties(fname=get_font_path())\n",
        "plt.xlabel(\"クラスタ\", fontproperties=fp)\n",
        "plt.ylabel(\"カテゴリー\", fontproperties=fp)\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.11"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}